# ğŸ§¬ Helical Model Container â€” Geneformer Fine-Tuning Environment

This directory defines a **Dockerized environment** for running the **Helical Geneformer** model locally or via Airflow. It encapsulates all necessary dependencies and scripts for a reproducible machine learning fine-tuning workflow.

---

## ğŸ“¦ Contents

The directory structure is as follows:

```

helical-model/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements-model.txt
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ run\_model.py
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample\_data.h5ad \# Your input dataset
â””â”€â”€ outputs/ \# Generated results

````

---

## âš™ï¸ Dockerfile Summary

The `Dockerfile` is optimized for reproducibility and build speed:

* **Base image:** `python:3.11-slim` (minimal and efficient)
* Installs core dependencies listed in `requirements-model.txt`.
* Copies application code and sets `/app` as the working directory.
* The default entrypoint runs the fine-tuning script: `python scripts/run_model.py`.

---

## ğŸ§© Script: `scripts/run_model.py`

### Overview

This script contains the core logic for the fine-tuning process:

* **Data Loading:** Loads the required local `.h5ad` dataset from `/app/data/sample_data.h5ad`.
* **Preprocessing:** Reduces the feature count (e.g., to **3000 genes**) to speed up the fine-tuning process for demonstration.
* **Fine-Tuning:** Initializes and fine-tunes the **Geneformer** model on the loaded data.
* **Print Metadata:** Outputs dataset shape and other relevant information to the console for verification.
* **Artifact Generation:** Generates the following output files:
    * `raw_logits.csv`
    * `predicted_celltypes.csv`
    * `fine_tuned_embeddings.npy`
    * `embedding_plot.png` (PCA visualization)
* **Storage:** Stores all outputs inside a unique, timestamped directory under `/app/outputs`: `outputs/sample_data_<timestamp>/`.

---

## ğŸš€ Usage

### 1ï¸âƒ£ Build the Image

Build the Docker image with the tag `helical-model:latest`:

```bash
docker build -t helical-model:latest .
````

### 2ï¸âƒ£ Run the Container Manually

You can test the environment by running the container directly, mounting the required local directories:

```bash
docker run -it --rm \
  -v "$(pwd)/data:/app/data" \
  -v "$(pwd)/outputs:/app/outputs" \
  -v "$(pwd)/scripts:/app/scripts" \
  helical-model
```

You should see logs similar to:

```sql
ğŸ“¥ Loading local dataset...
âœ… Loaded dataset with shape: (...)
ğŸ‰ Fine-tuning complete â€” outputs generated successfully!
```

-----

## ğŸ§  Features

âœ… Uses local data only (no online download needed)
âœ… Generates timestamped output directories to prevent overwriting
âœ… Easily integrates with Airflow via shared network
âœ… Minimal dependencies for fast build and reproducibility

### ğŸ” Example Output Structure

After a successful run, the `outputs/` directory will look like this:

```markdown
outputs/
â””â”€â”€ sample_data_20251112_174512/
    â”œâ”€â”€ raw_logits.csv
    â”œâ”€â”€ predicted_celltypes.csv
    â”œâ”€â”€ fine_tuned_embeddings.npy
```

-----

## ğŸ§­ Notes

  * Each run produces a new timestamped directory to **preserve historical results**.
  * The container can be triggered manually (as shown above) or via **Airflow's DockerOperator** for automated orchestration.
  * For faster iteration, dependencies are **cached** in the Docker image layersâ€”no need to reinstall dependencies between runs.

### ğŸ§© Next Steps

  * Log execution metrics (time taken, loss, accuracy, etc.) from `run_model.py`.
  * Extend the pipeline with post-processing or evaluation scripts to be run in subsequent Airflow tasks.